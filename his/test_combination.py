import pyautogui
import cv2
import numpy as np
import time
import os
import base64
from PIL import Image
from openai import OpenAI

# ============================================
# ä¸€ã€æˆªå›¾ä¸ç¿»é¡µéƒ¨åˆ†
# ============================================
SAVE_DIR = "image"                     # æˆªå›¾ä¿å­˜æ–‡ä»¶å¤¹
os.makedirs(SAVE_DIR, exist_ok=True)

BUTTON_IMAGE = "next_button.png"       # ç¿»é¡µæŒ‰é’®æˆªå›¾æ–‡ä»¶
CONFIDENCE = 0.8                       # æŒ‰é’®è¯†åˆ«ç›¸ä¼¼åº¦
SCREEN_REGION = None                   # æˆªå±èŒƒå›´ï¼ˆNone = å…¨å±ï¼‰
SIMILARITY_THRESHOLD = 0.99            # åˆ¤æ–­æˆªå›¾ç›¸ä¼¼åº¦ï¼ˆç”¨äºæ£€æµ‹åˆ°åº•ï¼‰

def take_screenshot(filename):
    """æˆªå±ä¿å­˜"""
    screenshot = pyautogui.screenshot(region=SCREEN_REGION)
    screenshot.save(filename)
    print(f"[+] æˆªå›¾å·²ä¿å­˜ï¼š{filename}")

def images_are_same(img1_path, img2_path, threshold=SIMILARITY_THRESHOLD):
    """åˆ¤æ–­ä¸¤å¼ æˆªå›¾æ˜¯å¦ç›¸åŒ"""
    img1 = cv2.imread(img1_path)
    img2 = cv2.imread(img2_path)
    if img1 is None or img2 is None or img1.shape != img2.shape:
        return False
    diff = cv2.absdiff(img1, img2)
    non_zero_count = np.count_nonzero(diff)
    total_pixels = diff.size / 3
    similarity = 1 - non_zero_count / total_pixels
    return similarity > threshold

def find_and_click_next():
    """è‡ªåŠ¨æŸ¥æ‰¾å¹¶ç‚¹å‡»â€œä¸‹ä¸€é¡µâ€æŒ‰é’®"""
    max_attempts = 5
    attempt = 0
    while attempt < max_attempts:
        button_location = pyautogui.locateOnScreen(BUTTON_IMAGE, confidence=CONFIDENCE)
        if button_location is not None:
            x, y = pyautogui.center(button_location)
            pyautogui.click(x, y)
            print(f"[â†’] ç‚¹å‡»ç¿»é¡µæŒ‰é’®ï¼Œåæ ‡ï¼š({x}, {y})")
            time.sleep(2)  # ç­‰å¾…é¡µé¢åŠ è½½
            pyautogui.moveTo(100, 200)
            pyautogui.click()
            return True
        else:
            attempt += 1
            print("æœªæ‰¾åˆ°ç¿»é¡µæŒ‰é’®ï¼Œé‡è¯•...")
            time.sleep(1)
    print("âš ï¸ ç¿»é¡µæŒ‰é’®æ‰¾ä¸åˆ°ï¼Œå¯èƒ½å·²åˆ°åº•ã€‚")
    return False

def capture_all_pages():
    """è‡ªåŠ¨æˆªå±å¹¶ä¿å­˜åˆ° image/ æ–‡ä»¶å¤¹"""
    print("å¼€å§‹è‡ªåŠ¨æˆªå± + è‡ªåŠ¨ç¿»é¡µæµç¨‹...")
    index = 0
    prev_img = os.path.join(SAVE_DIR, f"page_{index}.png")
    take_screenshot(prev_img)

    while True:
        if not find_and_click_next():
            break
        index += 1
        curr_img = os.path.join(SAVE_DIR, f"page_{index}.png")
        take_screenshot(curr_img)

        if images_are_same(prev_img, curr_img):
            print("æ£€æµ‹åˆ°ä¸¤å¼ æˆªå›¾ç›¸åŒï¼Œå·²åˆ°åº•ã€‚ç¨‹åºç»“æŸã€‚")
            os.remove(curr_img)
            break
        else:
            prev_img = curr_img

    print(f"âœ… æˆªå›¾å®Œæˆï¼Œå…± {index+1} é¡µã€‚")


# ============================================
# äºŒã€AIå›¾ç‰‡åˆ†æéƒ¨åˆ†
# ============================================
def analyze_images_with_ai():
    """ä» image/ æ–‡ä»¶å¤¹è¯»å–æ‰€æœ‰å›¾ç‰‡å¹¶ä¸€æ¬¡æ€§å‘ç»™ AI åˆ†æ"""
    client = OpenAI(
        api_key="sk-e80d1c4eec44443291dcc5191271d5c1",  # âš ï¸ è¯·æ›¿æ¢ä¸ºä½ è‡ªå·±çš„ API Key
        base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
    )

    image_files = sorted([f for f in os.listdir(SAVE_DIR) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])
    if not image_files:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡ï¼Œè¯·å…ˆè¿è¡Œæˆªå›¾éƒ¨åˆ†ã€‚")
        return

    print(f"ğŸ“¸ æ£€æµ‹åˆ° {len(image_files)} å¼ å›¾ç‰‡ï¼Œå¼€å§‹æ‰¹é‡åˆ†æ...\n")

    # æ„å»ºæ¶ˆæ¯å†…å®¹ï¼ŒåŒ…å«æ‰€æœ‰å›¾ç‰‡
    content_list = []
    
    # æ·»åŠ æ‰€æœ‰å›¾ç‰‡
    for i, img_name in enumerate(image_files):
        img_path = os.path.join(SAVE_DIR, img_name)
        with open(img_path, "rb") as f:
            img_base64 = base64.b64encode(f.read()).decode("utf-8")
        
        content_list.append({
            "type": "image_url",
            "image_url": {"url": "data:image/png;base64," + img_base64}
        })
        print(f"ğŸ“· å·²åŠ è½½ç¬¬ {i+1} å¼ å›¾ç‰‡ï¼š{img_name}")

    # æ·»åŠ æ–‡æœ¬æç¤º
    content_list.append({
        "type": "text",
        "text": f"è¿™æ˜¯2025å¹´ä¸­å›½ç§‘å­¦æŠ€æœ¯å¤§å­¦æ´»åŠ¨å‘å¸ƒå¹³å°çš„ç•Œé¢ï¼Œå…±{len(image_files)}å¼ å›¾ç‰‡ã€‚\
            è¯·æ€»ç»“å›¾ç‰‡ä¸­çš„æ´»åŠ¨ï¼Œå¹¶ä»¥è¡¨æ ¼çš„å½¢å¼åˆ—å‡ºã€‚\
            æ ¼å¼è¦æ±‚ï¼š\
            1.æ ¼å¼ï¼š| è§’æ ‡ | æ´»åŠ¨åç§° | ç»„ç»‡æ–¹ | æŠ¥åæˆªæ­¢æ—¶é—´ | \
            2.åŒæ—¶å°†æ¯ä¸ªæ´»åŠ¨æ—çš„â€œå¾·â€â€œæ™ºâ€â€œä½“â€â€œç¾â€â€œåŠ³â€äº”ç§è§’æ ‡ä½“ç°åœ¨è¡¨æ ¼ä¸­\
            è¯·ä»¥è¡¨æ ¼çš„å½¢å¼æ€»ç»“æ‰€æœ‰å›¾ç‰‡ä¸­çš„æ´»åŠ¨ä¿¡æ¯ï¼Œå°†æ‰€æœ‰æ´»åŠ¨åˆå¹¶åœ¨ä¸€ä¸ªå®Œæ•´çš„è¡¨æ ¼ä¸­ï¼Œä¸éœ€è¦ä»»ä½•å…¶ä»–è¾“å‡ºã€‚"
    })

    print(f"ğŸ§  æ­£åœ¨ä¸€æ¬¡æ€§åˆ†æ {len(image_files)} å¼ å›¾ç‰‡...")
    
    completion = client.chat.completions.create(
        model="qwen3-vl-flash",
        messages=[
            {
                "role": "user",
                "content": content_list
            }
        ],
        stream=False,
        extra_body={
            "enable_thinking": False,
            "thinking_budget": 81920
        },
    )

    print("=" * 50 + " åˆ†æç»“æœ " + "=" * 50)

    # å…¼å®¹ Qwen è¿”å›æ ¼å¼ï¼ˆå¯èƒ½æ˜¯ str æˆ– listï¼‰
    content = completion.choices[0].message.content
    if isinstance(content, str):
        print(content)
    elif isinstance(content, list):
        for part in content:
            if isinstance(part, dict) and part.get("text"):
                print(part["text"])
    else:
        print("ï¼ˆæ— è¾“å‡ºï¼‰")

    print("\nâœ… æ‰¹é‡å›¾ç‰‡åˆ†æå®Œæˆï¼")



# ============================================
# ä¸‰ã€ä¸»ç¨‹åºå…¥å£
# ============================================
if __name__ == "__main__":
    # print("ç¨‹åºå°†åœ¨ 5 ç§’åå¼€å§‹ï¼Œè¯·åˆ‡æ¢åˆ°éœ€è¦æˆªå›¾çš„çª—å£...")
    # time.sleep(5)

    # # Step 1: è‡ªåŠ¨æˆªå›¾å¹¶ç¿»é¡µ
    # capture_all_pages()

    # Step 2: AI æ‰¹é‡è¯†å›¾åˆ†æ
    analyze_images_with_ai()