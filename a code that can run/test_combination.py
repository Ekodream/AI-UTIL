import pyautogui
import cv2
import numpy as np
import time
import os
import base64
from PIL import Image
from openai import OpenAI

# ============================================
# ä¸€ã€æˆªå›¾ä¸ç¿»é¡µéƒ¨åˆ†
# ============================================
SAVE_DIR = "image"                     # æˆªå›¾ä¿å­˜æ–‡ä»¶å¤¹
os.makedirs(SAVE_DIR, exist_ok=True)

BUTTON_IMAGE = "next_button.png"       # ç¿»é¡µæŒ‰é’®æˆªå›¾æ–‡ä»¶
CONFIDENCE = 0.8                       # æŒ‰é’®è¯†åˆ«ç›¸ä¼¼åº¦
SCREEN_REGION = None                   # æˆªå±èŒƒå›´ï¼ˆNone = å…¨å±ï¼‰
SIMILARITY_THRESHOLD = 0.99            # åˆ¤æ–­æˆªå›¾ç›¸ä¼¼åº¦ï¼ˆç”¨äºæ£€æµ‹åˆ°åº•ï¼‰

def take_screenshot(filename):
    """æˆªå±ä¿å­˜"""
    screenshot = pyautogui.screenshot(region=SCREEN_REGION)
    screenshot.save(filename)
    print(f"[+] æˆªå›¾å·²ä¿å­˜ï¼š{filename}")

def images_are_same(img1_path, img2_path, threshold=SIMILARITY_THRESHOLD):
    """åˆ¤æ–­ä¸¤å¼ æˆªå›¾æ˜¯å¦ç›¸åŒ"""
    img1 = cv2.imread(img1_path)
    img2 = cv2.imread(img2_path)
    if img1 is None or img2 is None or img1.shape != img2.shape:
        return False
    diff = cv2.absdiff(img1, img2)
    non_zero_count = np.count_nonzero(diff)
    total_pixels = diff.size / 3
    similarity = 1 - non_zero_count / total_pixels
    return similarity > threshold

def find_and_click_next():
    """è‡ªåŠ¨æŸ¥æ‰¾å¹¶ç‚¹å‡»â€œä¸‹ä¸€é¡µâ€æŒ‰é’®"""
    max_attempts = 5
    attempt = 0
    while attempt < max_attempts:
        button_location = pyautogui.locateOnScreen(BUTTON_IMAGE, confidence=CONFIDENCE)
        if button_location is not None:
            x, y = pyautogui.center(button_location)
            pyautogui.click(x, y)
            print(f"[â†’] ç‚¹å‡»ç¿»é¡µæŒ‰é’®ï¼Œåæ ‡ï¼š({x}, {y})")
            time.sleep(2)  # ç­‰å¾…é¡µé¢åŠ è½½
            pyautogui.moveTo(100, 200)
            return True
        else:
            attempt += 1
            print("æœªæ‰¾åˆ°ç¿»é¡µæŒ‰é’®ï¼Œé‡è¯•...")
            time.sleep(1)
    print("âš ï¸ ç¿»é¡µæŒ‰é’®æ‰¾ä¸åˆ°ï¼Œå¯èƒ½å·²åˆ°åº•ã€‚")
    return False

def capture_all_pages():
    """è‡ªåŠ¨æˆªå±å¹¶ä¿å­˜åˆ° image/ æ–‡ä»¶å¤¹"""
    print("å¼€å§‹è‡ªåŠ¨æˆªå± + è‡ªåŠ¨ç¿»é¡µæµç¨‹...")
    index = 0
    prev_img = os.path.join(SAVE_DIR, f"page_{index}.png")
    take_screenshot(prev_img)

    while True:
        if not find_and_click_next():
            break
        index += 1
        curr_img = os.path.join(SAVE_DIR, f"page_{index}.png")
        take_screenshot(curr_img)

        if images_are_same(prev_img, curr_img):
            print("æ£€æµ‹åˆ°ä¸¤å¼ æˆªå›¾ç›¸åŒï¼Œå·²åˆ°åº•ã€‚ç¨‹åºç»“æŸã€‚")
            os.remove(curr_img)
            break
        else:
            prev_img = curr_img

    print(f"âœ… æˆªå›¾å®Œæˆï¼Œå…± {index+1} é¡µã€‚")


# ============================================
# äºŒã€AIå›¾ç‰‡åˆ†æéƒ¨åˆ†
# ============================================
def analyze_images_with_ai():
    """ä» image/ æ–‡ä»¶å¤¹è¯»å–å›¾ç‰‡å¹¶æ‰¹é‡å‘ç»™ AI åˆ†æ"""
    client = OpenAI(
        api_key="sk-xxx",  # âš ï¸ è¯·æ›¿æ¢ä¸ºä½ è‡ªå·±çš„ API Key
        base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
    )

    image_files = sorted([f for f in os.listdir(SAVE_DIR) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])
    if not image_files:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡ï¼Œè¯·å…ˆè¿è¡Œæˆªå›¾éƒ¨åˆ†ã€‚")
        return

    print(f"ğŸ“¸ æ£€æµ‹åˆ° {len(image_files)} å¼ å›¾ç‰‡ï¼Œå¼€å§‹é€å¼ åˆ†æ...\n")

    for i, img_name in enumerate(image_files):
        img_path = os.path.join(SAVE_DIR, img_name)
        with open(img_path, "rb") as f:
            img_base64 = base64.b64encode(f.read()).decode("utf-8")

        print(f"ğŸ§  æ­£åœ¨åˆ†æç¬¬ {i+1}/{len(image_files)} å¼ å›¾ç‰‡ï¼š{img_name}")
        completion = client.chat.completions.create(
            model="qwen3-vl-flash",
            messages=[
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "image_url",
                            "image_url": {"url": "data:image/png;base64," + img_base64}
                        },
                        {
                            "type": "text",
                            "text": "è¯·æ€»ç»“å›¾ç‰‡ä¸­æœ‰å“ªäº›æ´»åŠ¨ã€‚å¦‚æœæˆ‘éœ€è¦æ™ºåŠ›æå‡ï¼Œæˆ‘åº”è¯¥å‚åŠ å“ªäº›æ´»åŠ¨ï¼Œå¹¶åˆ—å‡ºæ—¶é—´å®‰æ’ï¼›å¦‚æœæ´»åŠ¨æ—¶é—´å†²çªï¼Œè¯·è¯´æ˜ã€‚"
                        },
                    ],
                },
            ],
            stream=False,
            extra_body={
                "enable_thinking": False,
                "thinking_budget": 81920
            },
        )

        print("=" * 20 + f" ç¬¬ {i+1} å¼ åˆ†æç»“æœ " + "=" * 20)

        # å…¼å®¹ Qwen è¿”å›æ ¼å¼ï¼ˆå¯èƒ½æ˜¯ str æˆ– listï¼‰
        content = completion.choices[0].message.content
        if isinstance(content, str):
            print(content)
        elif isinstance(content, list):
            for part in content:
                if isinstance(part, dict) and part.get("text"):
                    print(part["text"])
        else:
            print("ï¼ˆæ— è¾“å‡ºï¼‰")

        print("\n")

        time.sleep(2)  # é¿å…è¯·æ±‚è¿‡å¿«

    print("âœ… æ‰€æœ‰å›¾ç‰‡åˆ†æå®Œæˆï¼")



# ============================================
# ä¸‰ã€ä¸»ç¨‹åºå…¥å£
# ============================================
if __name__ == "__main__":
    print("ç¨‹åºå°†åœ¨ 5 ç§’åå¼€å§‹ï¼Œè¯·åˆ‡æ¢åˆ°éœ€è¦æˆªå›¾çš„çª—å£...")
    time.sleep(5)

    # Step 1: è‡ªåŠ¨æˆªå›¾å¹¶ç¿»é¡µ
    capture_all_pages()

    # Step 2: AI æ‰¹é‡è¯†å›¾åˆ†æ
    analyze_images_with_ai()
